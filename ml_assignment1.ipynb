{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6881c3fe-6a9d-405f-a831-4e7fb1a15eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ques 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a1350fa-5a66-481d-9cc5-65cfc4b98d6d",
   "metadata": {},
   "source": [
    "Artificial Intelligence (AI) refers to the simulation of human intelligence in machines programmed to think, reason, learn, and perform tasks autonomously. AI encompasses tasks such as decision-making, problem-solving, natural language processing, and perception."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ffd075-9ae3-4061-9e22-f6d4773dacfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ques 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ee565c4-b7ef-4a24-a459-d9a1c5b98b2a",
   "metadata": {},
   "source": [
    "Artificial Intelligence (AI), Machine Learning (ML), Deep Learning (DL), and Data Science (DS) are interconnected but distinct fields. \n",
    "AI is the overarching field focused on creating systems that mimic human intelligence to perform tasks like decision-making and automation. ML is a subset of AI that enables machines to learn patterns from data and improve their performance without explicit programming.\n",
    "DL, a further subset of ML, uses neural networks to process vast amounts of data and excels in tasks like image recognition and natural language processing.\n",
    "Data Science, on the other hand, is a multidisciplinary field that combines statistics, programming, and domain expertise to extract insights and build models using data.\n",
    "While AI emphasizes automation and decision-making, ML and DL focus on learning from data, and DS provides the foundation for analyzing and processing data to inform decision-making or power AI systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec4eb13-907c-43a5-a117-9382f2003a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ques 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf65abbf-d6e8-40c0-8e26-4b55d89754eb",
   "metadata": {},
   "source": [
    "Artificial Intelligence (AI) differs significantly from traditional software development in its approach and functionality. Traditional software development relies on pre-defined, rule-based logic explicitly programmed by developers, making the software static and task-specific unless manually updated. In contrast, AI systems are designed to learn from data, adapt over time, and improve their performance without human intervention. While traditional software processes fixed inputs to produce predictable outputs, AI systems dynamically adjust based on patterns and trends in the data they analyze. This adaptability enables AI to handle complex tasks like fraud detection, natural language processing, and image recognition, which would be impractical or impossible with traditional software development methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db598ce4-c483-4cb9-81dc-673e3b46e280",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ques 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "249f7f4f-eb3c-4f64-b3d5-a2691cd7101e",
   "metadata": {},
   "source": [
    "AI:  Virtual assistants (Siri, Alexa), robotics, fraud detection.\n",
    "ML: Recommendation systems (Netflix, Amazon), spam email detection.\n",
    "DL:Self-driving cars, facial recognition, speech translation.\n",
    "DS:Customer segmentation, stock market prediction, healthcare analytics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "475fe480-6d15-4275-bc47-5f2f51bf17fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ques 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0d7c87a-ecc8-4bf1-94d2-ab1b5143c3b4",
   "metadata": {},
   "source": [
    "AI:Enhances automation, improves efficiency, and drives innovation across industries.\n",
    "ML:Enables personalized experiences, fraud detection, and predictive analytics.\n",
    "DL:Powers complex applications like autonomous vehicles, medical imaging, and natural language processing.\n",
    "DS:Helps organizations make data-driven decisions, optimize strategies, and uncover insights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be3002c-4860-4004-a676-849be0d05c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ques 6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "044bb232-30d5-460f-b327-8db10e4e4b57",
   "metadata": {},
   "source": [
    "Supervised learning is a type of machine learning where the algorithm is trained on labeled data. The model learns to map inputs to the correct outputs based on this training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "169d308d-9dc3-4866-a8f6-6809f495f3b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ques 7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f82a4a47-db74-41a5-8a66-cc5aaa418ca0",
   "metadata": {},
   "source": [
    "1.Linear Regression\n",
    "2.Logistic Regression\n",
    "3.Decision Trees\n",
    "4.Random Forest\n",
    "5.Support Vector Machines (SVM)\n",
    "6.Neural Networks (for structured data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d0c9f60-e500-4f20-9461-f26abcd81fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ques 8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de47c4bf-46bc-413a-a50c-199cc06266b2",
   "metadata": {},
   "source": [
    "1.Data collection: Gather labeled data (input-output pairs).\n",
    "2.Data Preprocessing: Clean, normalize, and split data into training and test sets.\n",
    "3.Model Selection: Choose a suitable supervised algorithm.\n",
    "4.Training:  Train the model using the training data.\n",
    "5.Validation: Optimize parameters using validation data.\n",
    "6.Evaluation: Evaluate the model's performance using metrics like accuracy, precision, recall.\n",
    "7.Deployment: Deploy the model for predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc92750-a416-49ca-924f-c90d46d8dfce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ques 9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06c9065a-d57f-464d-8224-5a75aea67849",
   "metadata": {},
   "source": [
    "1.No labeled data is required.\n",
    "2.Finds hidden patterns or structures in data.\n",
    "3.Often used for clustering or dimensionality reduction.\n",
    "4.Results may require human interpretation.\n",
    "5.Works well for exploratory data analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "704cc52a-7491-405c-8ea8-2bde4f21826f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ques 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8fcc6ea-c7a3-4a4d-9976-a8bab406261f",
   "metadata": {},
   "source": [
    "1.K-Means Clustering\n",
    "2.Hierarchical Clustering\n",
    "3.Principal Component Analysis (PCA)\n",
    "4.Autoencoders\n",
    "5.DBSCAN (Density-Based Spatial Clustering of Applications with Noise)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d46a6772-3094-4303-adca-116dee96f7fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ques 11"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0089f27-61ef-452f-a601-b7dc03014e1a",
   "metadata": {},
   "source": [
    "Semi-Supervised Learning is a machine learning approach that uses a combination of labeled and unlabeled data for training, with the majority of the dataset being unlabeled. It bridges the gap between supervised and unsupervised learning, leveraging a small amount of labeled data to improve learning accuracy while reducing the cost and effort of labeling large datasets. This method is particularly significant in scenarios like speech recognition, where labeled data is scarce and expensive to obtain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86118f9c-cc2f-4d66-a605-e1dee30ac648",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ques 12"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a641e1-662c-4cde-8cca-dc4787fd776e",
   "metadata": {},
   "source": [
    "Reinforcement Learning (RL) is a machine learning paradigm where an agent learns to make decisions by interacting with an environment to maximize a cumulative reward. The agent explores various actions and learns from feedback (rewards or penalties) to optimize its behavior over time. RL has applications in robotics (autonomous control), gaming (e.g., AlphaGo), finance (portfolio optimization), healthcare (personalized treatment), and self-driving cars."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "437fe0ab-3541-45ae-935a-ad3ad7c10f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ques 13"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e07b61-6e20-408e-ac21-64ab5b3a39e4",
   "metadata": {},
   "source": [
    "Reinforcement Learning differs fundamentally from both supervised and unsupervised learning. In supervised learning, the model learns from labeled data, with explicit input-output pairs provided for training. In unsupervised learning, the model discovers patterns or structures in unlabeled data. In contrast, RL involves an agent learning through trial and error by interacting with an environment, receiving feedback in the form of rewards or penalties, and optimizing its actions to achieve long-term objectives. Unlike supervised learning, RL does not require labeled data, and unlike unsupervised learning, it focuses on decision-making and goal-directed learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d3dbf0-89c7-499f-8c5e-1442ec40da2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ques 14"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c163b5b5-ae90-44f5-9485-87ea5ca6daca",
   "metadata": {},
   "source": [
    "The train-test-validation split is essential for building and evaluating machine learning models. The training set is used to train the model by enabling it to learn patterns and relationships in the data. The validation set helps tune hyperparameters and prevent overfitting by assessing the model’s performance during training. The test set evaluates the final performance of the trained model on unseen data, providing an unbiased estimate of its generalization ability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8bd7809-5241-4931-a6e2-5b50015c287f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ques 15"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2261b902-307b-4d42-89ce-d0c143902e9d",
   "metadata": {},
   "source": [
    "The training set is the foundation for developing a machine learning model, as it contains the labeled data used to learn the patterns and relationships necessary for making predictions. The quality and quantity of the training data directly influence the model's accuracy and ability to generalize to new data. A diverse and representative training set ensures the model performs well across various scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e94c78-44a9-4311-af6f-fd04b9d07508",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ques 16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43a7b615-b807-480e-89c5-0e5d9776607b",
   "metadata": {},
   "source": [
    "The size of each split depends on the dataset size and the problem being addressed. A common split ratio is 70-80% for training, 10-15% for validation, and 10-20% for testing. For small datasets, cross-validation can be used to maximize the use of available data. The goal is to ensure enough data for training while retaining sufficient data for validation and testing to provide reliable evaluations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a9fdbaf-9e77-4d50-b4c5-46da079a79cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ques 17"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "996b0b67-2c0e-437f-9904-41d5a60643e8",
   "metadata": {},
   "source": [
    "The size of each split depends on the dataset size and the problem being addressed. A common split ratio is 70-80% for training, 10-15% for validation, and 10-20% for testing. For small datasets, cross-validation can be used to maximize the use of available data. The goal is to ensure enough data for training while retaining sufficient data for validation and testing to provide reliable evaluations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4cfe896-bd08-41c5-935e-f0ad8946a55f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ques 18"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae82ee65-f8f5-4023-9a3c-a366fc5a0e4b",
   "metadata": {},
   "source": [
    "Selecting split ratios involves balancing the need for sufficient training data to learn patterns and enough validation and test data to evaluate the model. Larger training sets improve learning, but smaller test sets may reduce confidence in performance estimates. Conversely, smaller training sets risk underfitting, while larger test sets might leave insufficient data for training. The trade-off depends on the dataset size and the complexity of the problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da6815a6-0aef-45d2-b073-aa27d3670593",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ques 19"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4be34c7-8abd-45e7-8e4b-0cfdc05e6dba",
   "metadata": {},
   "source": [
    "Model performance refers to how well a machine learning model predicts outcomes or classifies data, typically measured by specific evaluation metrics. High performance indicates the model generalizes well to new, unseen data, while poor performance suggests overfitting, underfitting, or inadequate data representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "432252e8-b667-4e59-9a7e-1cc73a313652",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ques 20"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bdeb4b0-27c2-4414-b0b0-deb84b637210",
   "metadata": {},
   "source": [
    "The performance of a machine learning model is measured using evaluation metrics such as:\n",
    "1.Accuracy\n",
    "2.Precision and recall\n",
    "3. F1 Score\n",
    "4. Mean Absolute Error\n",
    "5.Root Mean Squared Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d85aaded-41fc-429c-bd92-336d3a43bca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ques 21"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d250bac0-c79b-4978-88a7-cf57e36f045c",
   "metadata": {},
   "source": [
    "Overfitting occurs when a machine learning model learns the noise or random fluctuations in the training data rather than the underlying patterns. This results in excellent performance on the training data but poor generalization to new, unseen data. Overfitting is problematic because the model becomes too specific to the training data, losing its ability to perform well in real-world scenarios, leading to inaccurate predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77553dad-4b73-4ca3-8690-5df499997a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ques 22"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26f5e104-edc2-4a50-8a74-01bfeef042a2",
   "metadata": {},
   "source": [
    "1. Increase training data\n",
    "2. Simplify the model\n",
    "3. Regularisation\n",
    "4. Early Stopping\n",
    "5. Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "115a721d-277b-4911-b67a-d39a015215fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ques 23"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ceb2643-fec6-4736-b0c6-55ddfe2052b5",
   "metadata": {},
   "source": [
    "Underfitting occurs when a model is too simple to capture the underlying patterns in the data, resulting in poor performance on both the training and test datasets. This happens when the model fails to learn sufficiently from the data, often due to insufficient complexity or inadequate training. Underfitting leads to inaccurate predictions and reduces the model's utility in solving the problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eb0e480-12fd-4a7b-80db-7737911bede9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ques 24"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3447fa5c-6608-4f0e-9ffa-ffda3c1a6cc2",
   "metadata": {},
   "source": [
    "1. Increase model complexity\n",
    "2. Train longer\n",
    "3. Add features\n",
    "4. REduce Regularization\n",
    "5. Optimize hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "488a4ca1-fe93-40ce-836f-e0d65d3fcb93",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ques 25"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59853cd4-66db-48a1-bfc3-0961c666079b",
   "metadata": {},
   "source": [
    "Bias: Refers to errors introduced by approximating a real-world problem with a simplified model. High bias results in underfitting, where the model cannot capture the data’s complexity.\n",
    "\n",
    "Variance:  Refers to the model's sensitivity to fluctuations in the training data. High variance results in overfitting, where the model performs well on training data but poorly on unseen data.\n",
    "\n",
    "Achieving the right balance involves creating a model that is neither too simple nor too complex. Techniques like cross-validation, regularization, and fine-tuning hyperparameters help in maintaining this balance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbcd39ce-4695-4172-92ef-c32a2c6bd4ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ques 26"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c31ce9f-c2eb-4ea7-8bff-92696dc325f5",
   "metadata": {},
   "source": [
    "1.Deletion\n",
    "2.Imputation\n",
    "3.Advanced Techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d47d1cbe-bb2b-4860-a456-4d9a2c53ead0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ques 27"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c09f55d-d374-44d3-82de-cee6d71e7ee5",
   "metadata": {},
   "source": [
    "Ignoring missing data can lead to biased results, reduced accuracy, and misrepresentation of the dataset. If missing data is not handled properly, it may distort relationships between variables and undermine the model’s ability to generalize. In severe cases, ignoring missing data can result in invalid conclusions or predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43db4416-265e-46c7-a7d5-a50720ef5df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ques 28"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "876626e2-a271-42bc-b78d-d9c69a6171d2",
   "metadata": {},
   "source": [
    "Pros:\n",
    "1.Mean/median/mode imputation: Simple and fast to implement.\n",
    "Works well for small amounts of missing data.\n",
    "2.Regression imputation: Considers relationships between variables, leading to more accurate estimates.\n",
    "3.Multiple imputation: Provides robust and reliable estimates by accounting for variability.\n",
    "\n",
    "Cons:\n",
    "1.Mean/median/mode imputation: Ignores relationships between variables and reduces variance in the dataset.\n",
    "2.Regression imputation:Can lead to overfitting if relationships are weak or improperly modeled.\n",
    "3.Multiple imputation: Complex to implement and computationally intensive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "466aab7c-dce0-4e35-99d2-275c92dce148",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ques 29"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1c5b29f-05a1-4cd6-9f35-d938df7debc7",
   "metadata": {},
   "source": [
    "Missing data can negatively impact model performance by:\n",
    "\n",
    "1.Reducing the amount of usable data, leading to biased estimates.\n",
    "2.Affecting the model’s ability to learn patterns, resulting in lower accuracy or reliability.\n",
    "3.Introducing noise or instability in predictions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77aeece9-17ee-40a2-aad7-1065524e6410",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ques 30"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a6c1b08-1b39-4171-96db-18734b93fa5e",
   "metadata": {},
   "source": [
    "Imbalanced data refers to datasets where the distribution of classes is uneven. For example, in binary classification, if one class constitutes 95% of the data while the other makes up only 5%, the dataset is imbalanced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78a5f796-a0d6-4bdd-bd47-1dad1803e55d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ques 31"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b17fe49-228a-4fae-b98c-3e07078f4a07",
   "metadata": {},
   "source": [
    "Challenges include:\n",
    "\n",
    "1.Models being biased toward the majority class.\n",
    "2.Poor performance metrics, such as high accuracy but low precision/recall for the minority class.\n",
    "3.Difficulty in detecting rare events or patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4cecc06-32d7-401e-91ef-675e10594375",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ques 32"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea57bbd3-e95f-44a4-ab50-4ba171b5beae",
   "metadata": {},
   "source": [
    "1. Data-level approaches\n",
    "2. Algorithm-level approaches\n",
    "3. Evaluation metrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a89cd11f-d371-4583-bc4b-9473a49edc62",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ques 33"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc66c1c-2eb0-41c2-b7ec-0740405b8dc5",
   "metadata": {},
   "source": [
    "Up-Sampling:  Increasing the number of samples in the minority class by duplicating existing data or generating synthetic samples.\n",
    "\n",
    "Down-Sampling: Reducing the number of samples in the majority class by randomly removing instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f8f49a0-f6d4-4160-b74d-7fb7ab81a757",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ques 34"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3557a94f-992b-4c57-b01a-532c8d1569e3",
   "metadata": {},
   "source": [
    "Up-Sampling: When you have limited data and removing majority-class samples might result in loss of information.\n",
    "Down-Sampling:  When you have a large dataset and removing some majority-class samples won't significantly affect learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7655480e-4208-48c9-9bfc-c1bbeb75611e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ques 35"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2ef76e5-4310-43ed-ab70-9b317c2eaa8b",
   "metadata": {},
   "source": [
    "SMOTE (Synthetic Minority Over-sampling Technique) generates synthetic samples for the minority class by interpolating between existing samples. It selects a minority-class sample, identifies its k-nearest neighbors, and creates new data points along the line segments joining them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5f84e65-63d9-4545-a475-9273fc0056e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ques 36"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9547ee3d-8fae-4a9d-b4f0-aa083481190d",
   "metadata": {},
   "source": [
    "SMOTE enhances the representation of the minority class without duplication, making models more balanced and reducing overfitting risks compared to simple duplication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c431d83-c7ea-4a66-a2f3-97e4781f5b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ques 37"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "076c3398-4170-4d31-be1f-2c3bf159af52",
   "metadata": {},
   "source": [
    "Advantages:\n",
    "1.Improves minority-class representation.\n",
    "2.Reduces overfitting compared to random oversampling.\n",
    "Limitations:\n",
    "1.May create noisy or unrealistic samples.\n",
    "2.Doesn't address imbalances in feature distributions or inter-class overlaps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "576c2478-7515-4ba7-bf7c-bcc8aee1fb2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ques 38"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17df8e5e-f340-48dc-82cc-72c402c5f574",
   "metadata": {},
   "source": [
    "1.Fraud detection in financial transactions.\n",
    "2.Disease diagnosis where positive cases are rare.\n",
    "3.Identifying defective items in quality control."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ecffa1c-8cbb-4309-8904-adad14cc1f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ques 39"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f273a85-9792-4ad5-8816-6f09602111fa",
   "metadata": {},
   "source": [
    "Data interpolation estimates unknown values within a dataset based on surrounding known values. It’s used to fill in missing data or create a continuous representation of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8bba870-272d-4b8f-855d-39764d31ac89",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ques 40"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c0e58e-d5d2-4aae-956d-bfb8c22d99eb",
   "metadata": {},
   "source": [
    "1.Linear interpolation.\n",
    "2.Polynomial interpolation.\n",
    "3.Spline interpolation.\n",
    "4.Nearest-neighbor interpolation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bef29d5-dd21-4c40-b87b-e6c968443a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ques 41"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3ded16e-98a2-4105-b88a-19a294ab4005",
   "metadata": {},
   "source": [
    "Positive implications: Retains dataset completeness, preventing data loss.\n",
    "Negative implications:  May introduce bias if assumptions about data patterns are incorrect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d59857-59d9-4396-96fb-37301ebbfe4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ques 42"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecc86e0d-e676-4aa2-b897-85e8a5fde6d3",
   "metadata": {},
   "source": [
    "Outliers are data points significantly different from the majority of the dataset, often lying far from the mean or median."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da78ff92-bf2c-4e14-b086-db0749f35214",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ques 43"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89d6d28a-45c5-492c-a121-e529b0dc02d7",
   "metadata": {},
   "source": [
    "Outliers can:\n",
    "\n",
    "1.Skew statistical summaries (mean, variance).\n",
    "2.Degrade model performance by misguiding learning algorithms.\n",
    "3.Increase the risk of overfitting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1ac78ae-7769-4dc0-ae58-dd7a9d14167d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ques 44"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4893fa1-ec1e-4a1f-b584-967804c34f8b",
   "metadata": {},
   "source": [
    "1.Statistical methods (e.g., Z-score, IQR).\n",
    "2.Visualization (e.g., box plots, scatter plots).\n",
    "3.Model-based methods (e.g., isolation forest, DBSCAN)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea0a86d-c863-449d-9104-39654d33ec6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ques 45"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61630002-5fc2-4512-ab6a-ed5b01de6e65",
   "metadata": {},
   "source": [
    "1. Removal\n",
    "2. Transformation\n",
    "3. Imputation\n",
    "4. Robust Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3cfb7c3-e866-4271-bba9-3b3f6a319036",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ques 46"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0989299-b2d9-455c-8469-7eb144d6eecb",
   "metadata": {},
   "source": [
    "Filter methods:\n",
    "1.Select features based on statistical metrics (e.g., correlation, mutual information).\n",
    "2.Fast but independent of the learning algorithm.\n",
    "\n",
    "Wrapper methods:\n",
    "1.Use a model to evaluate feature subsets (e.g., forward selection, backward elimination).\n",
    "2.More accurate but computationally expensive.\n",
    "\n",
    "Embedded methods:\n",
    "1.Feature selection occurs during model training (e.g., LASSO, decision trees).\n",
    "2.Balances accuracy and computational efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d78e57b-9d3b-4e22-b19e-ee7dffb9e4b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ques 47"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61086846-3d9e-48d0-8a3d-84edf8b4ecce",
   "metadata": {},
   "source": [
    "Filter methods:Correlation matrix, chi-squared test, mutual information.\n",
    "Wrapper methods:Recursive Feature Elimination (RFE), forward selection, backward elimination.\n",
    "Embedded methods:LASSO (L1 regularization), decision tree-based algorithms (e.g., Random Forest, XGBoost)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fdb56c3-9821-4135-b987-983b08febe9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ques 48"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8225dd6d-e36d-4c79-a269-21268e27c9cc",
   "metadata": {},
   "source": [
    "1.Filter methods:\n",
    "Advantages: Simple, fast, independent of model type\n",
    "Disadvantages:Doesn’t consider interactions between features and the model.\n",
    "\n",
    "2.Wrapper methods:\n",
    "Advantages:  Provides better subsets as they are optimized for a specific model.\n",
    "Disadvantages: Computationally expensive, prone to overfitting for small datasets\n",
    "\n",
    "3.Embedded methods:\n",
    "Advantages:Efficient, selects features while training the model.\n",
    "Disadvantages:  Depends on the model, can be biased by regularization strength.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a61572-a31d-4c12-8cd5-af3dffd43929",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ques 49"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75652871-6743-4c5a-85c4-c9f0a072fbdb",
   "metadata": {},
   "source": [
    "Feature scaling transforms features to a similar scale to improve model performance, particularly for algorithms sensitive to feature magnitudes (e.g., SVM, KNN)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb6b1491-1a82-49d1-8ffc-b627443d6bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ques 50"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7576eb5b-814f-4843-b779-6cf0815fbb1f",
   "metadata": {},
   "source": [
    "Standardization (z-score normalization) rescales data by centering it around the mean and scaling it based on standard deviation:\n",
    "Z= (X - mu)/sigma\n",
    "\n",
    "where mu is the mean and sigma is the standard deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "092782ef-1525-4a14-834c-a5ba4f584125",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ques 51"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "192d1218-b7c5-4d74-b12f-b592ce1b9169",
   "metadata": {},
   "source": [
    "Mean normalization:\n",
    "X' = (X - mu)/ (Xmax - Xmin)\n",
    "Standardization:\n",
    "Centers data at 0 with unit variance, without bounding it to a specific range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb40cf02-b760-4360-a1cc-b346be0d5ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ques 52"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "449321d2-f8d9-4dfc-8df1-60df35e3c268",
   "metadata": {},
   "source": [
    "Advantages:\n",
    "Retains original data distribution.\n",
    "Useful for algorithms requiring bounded ranges (e.g., neural networks).\n",
    "\n",
    "Disadvantages:\n",
    "Sensitive to outliers, which can distort the range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e64ffc0c-738d-423c-b72c-bfcbf1b0c79e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ques 53"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8cbecf6-86a9-4bbc-b385-8e16764e15dc",
   "metadata": {},
   "source": [
    "Unit vector scaling normalizes each feature vector to a unit norm (e.g., length = 1). It’s useful when relative feature magnitudes matter more than their absolute values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "733ad7bc-e71e-45ed-810b-133e6711c6a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ques 54"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "014de258-9a38-45fc-8833-2d223f7f0e5e",
   "metadata": {},
   "source": [
    "PCA is a dimensionality reduction technique that transforms correlated features into uncorrelated ones (principal components), ordered by the amount of variance they capture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "316364ca-7916-421a-97ff-af8756160cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ques 55"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0824f507-a088-4111-b157-65702570244f",
   "metadata": {},
   "source": [
    "1.Standardize the dataset.\n",
    "2.Compute the covariance matrix.\n",
    "3.Calculate eigenvalues and eigenvectors of the covariance matrix.\n",
    "4.Select the top-k principal components based on eigenvalues.\n",
    "5.Project data onto the selected components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bcaa847-2fdc-42e1-aa89-65f628dbca7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ques 56"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d95dd032-a645-4172-ba26-180752ff9ef3",
   "metadata": {},
   "source": [
    "Eigenvalues: Indicate the variance explained by each principal component.\n",
    "Eigenvectors:  Define the direction of the principal components in feature space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fa72fbb-d3fb-4365-83e0-23e15768f6e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ques 57"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1c169c2-c952-499d-a287-bf8e9cab9769",
   "metadata": {},
   "source": [
    "PCA reduces dimensions by projecting data onto a smaller number of principal components, preserving maximum variance and eliminating redundant or less important features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38d734e2-ae54-4b38-8507-c950e7f436cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ques 58"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24715d1e-d433-4391-87e3-136a692b6ee1",
   "metadata": {},
   "source": [
    "Data encoding converts categorical variables into numerical representations to enable compatibility with machine learning algorithms, which typically require numerical inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d80af165-c38d-46b8-a659-dd6d84eb88f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ques 59"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "738227d6-e676-4393-a62a-75d88e711cc0",
   "metadata": {},
   "source": [
    "Nominal Encoding: Represents categorical data using one-hot encoding, label encoding, or binary encoding.\n",
    "Example:\n",
    "For a \"Color\" feature with values {Red, Blue, Green}, one-hot encoding creates:\n",
    "Red: [1,0,1]\n",
    "Blue: [0, 1, 0]\n",
    "Green: [0, 0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c4721d6-f28e-4e06-95e6-3123dbef2241",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ques 60"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6c33bc7-4315-42d8-8e7e-55fa29b9d757",
   "metadata": {},
   "source": [
    "One Hot Encoding transforms categorical variables into binary vectors for machine learning models. Each category is represented as a binary column, with a value of 1 indicating the presence of the category and 0 indicating absence.\n",
    "1.Identify all unique categories for a feature.\n",
    "2.Create new binary columns for each category.\n",
    "3.Assign 1 to the column corresponding to the category of a data point and 0 to all others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8b5015a-4238-497e-a6b7-bb13285fa8bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ques 61"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffa0cc48-38a6-43a0-88fa-66507ca02fb9",
   "metadata": {},
   "source": [
    "If a feature has many categories (e.g., >50), one hot encoding may:\n",
    "Create Sparse data:  High-dimensional data with mostly zeros.\n",
    "Solutions:\n",
    "Use dimensionality reduction after encoding.\n",
    "Apply Frequency Encoding or Target Encoding instead.\n",
    "Group rare categories into a single \"Other\" category.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f497ac-8c05-4db6-8f0b-22dcd18625e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ques 62"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "485d52a2-fc35-4859-9ca7-42dc9e7f1db0",
   "metadata": {},
   "source": [
    " Replaces each category with the mean of the target variable for that category.\n",
    "\n",
    " Advantages:\n",
    " Captures category-to-target relationships.\n",
    "Reduces dimensionality compared to one hot encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac991ef-e99c-4e2e-a3e9-580ce9a77a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ques 63"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1c51941-518b-4e86-999c-4cb773f2d4ed",
   "metadata": {},
   "source": [
    "Ordinal Encoding:Assigns integers to categories with a meaningful order.\n",
    "Example: Education Levels {High School, Bachelor's, Master's, PhD} → {1, 2, 3, 4}.\n",
    "\n",
    "label Encoding: Assigns integers to categories without an inherent order.\n",
    "Example: Colors {Red, Blue, Green} → {0, 1, 2}."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd7a820b-62dd-4608-a6be-224d5322a43d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ques 64"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65cdd3a2-ded2-4488-be76-908eaba84bf9",
   "metadata": {},
   "source": [
    "This technique orders categories based on their relationship to the target variable.\n",
    "\n",
    "Steps:\n",
    "\n",
    "1.Calculate the mean of the target variable for each category.\n",
    "2.Assign ranks based on these means.\n",
    "Example:\n",
    "For a \"Country\" feature predicting sales:\n",
    "\n",
    "Country A → Avg. Sales: 300 → Rank 1.\n",
    "Country B → Avg. Sales: 200 → Rank 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78780294-d001-4bb4-84b2-c0f00117c2f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ques 65"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6ac1a13-1943-4a7a-ab9b-3977d39a0a29",
   "metadata": {},
   "source": [
    "Covariance measures the degree to which two variables change together.\n",
    "Significance:\n",
    "Positive covariance → Variables move in the same direction.\n",
    "Negative covariance → Variables move in opposite directions.\n",
    "Zero covariance → No linear relationship.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d02d2fe-e904-4736-9ce2-73ec05849a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ques 66"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9351c800-92df-417f-9d57-0b76b686716e",
   "metadata": {},
   "source": [
    "Steps:\n",
    "1.Calculate pairwise correlation coefficients (e.g., Pearson, Spearman).\n",
    "2.Visualize with a correlation matrix or heatmap.\n",
    "3.Identify highly correlated features (e.g., correlation > 0.8).\n",
    "4.Drop redundant features or use dimensionality reduction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cec942ab-15d9-4c02-8846-1090f44446dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ques 67"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df40333c-adc1-45ad-8b21-cdbb0a9ef8b8",
   "metadata": {},
   "source": [
    "Measures linear relationship between two variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "327eddbd-0eef-49a9-98a7-e51d2cadfeae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ques 68"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de7a0baa-e525-4396-b3af-23d58f8db664",
   "metadata": {},
   "source": [
    "Pearson's Correlation: Measures linear relationships; sensitive to outliers.\n",
    "Spearman's Rank Correlation: Measures monotonic relationships (not necessarily linear) using ranks; robust to outliers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9033955-1d09-46fd-a769-8546461f0c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ques 69"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "957e05de-9170-4c79-bf66-bd1a57c05078",
   "metadata": {},
   "source": [
    "VIF quantifies multicollinearity among features:\n",
    "Importance:\n",
    "1.High VIF (>10) indicates multicollinearity, which can distort model coefficients.\n",
    "2.Features with high VIF should be dropped or combined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "653e45e4-ee8e-4c4b-a30a-566babc3076d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ques 70"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90f75174-467a-4190-9917-2f29f52ca09c",
   "metadata": {},
   "source": [
    "Feature selection identifies the most relevant features for a model to improve performance by:\n",
    "\n",
    "1.Reducing overfitting.\n",
    "2.Decreasing computational complexity.\n",
    "3.Enhancing interpretability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f45d42e6-1e9a-4547-91fe-ec7bbfd79b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ques 71"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59a2b93f-5fe2-401e-af96-7ed0729d3baf",
   "metadata": {},
   "source": [
    "RFE is a wrapper method for feature selection:\n",
    "\n",
    "1.Train a model and rank features by importance.\n",
    "2.Iteratively remove the least important feature(s).\n",
    "3.Stop when the desired number of features is reached."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4579e08a-eba2-4c2f-b54e-f42eb8616950",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ques 72"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af45d20a-574c-4946-a46c-3559ba2d8e36",
   "metadata": {},
   "source": [
    "Backward Elimination removes features iteratively:\n",
    "\n",
    "1.Start with all features in the model.\n",
    "2.Remove the least statistically significant feature (e.g., highest p-value > 0.05).\n",
    "3.Repeat until all features are statistically significant.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad73a000-e9ce-43ec-a146-9986d5719541",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ques 73"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdad6289-f34e-4421-a8c8-0b95c3f6b21a",
   "metadata": {},
   "source": [
    "Advantages:\n",
    "1.Starts with no features, building an optimal subset.\n",
    "2.Useful for small datasets.\n",
    "Limitations:\n",
    "1.Computationally expensive for large feature sets.\n",
    "2.Prone to local optima as it doesn’t revisit previously excluded features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1d3a31a-43d2-493d-b8ae-40bfff409efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ques 74"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17a39ec4-c89b-470c-bbcb-111f6b59885f",
   "metadata": {},
   "source": [
    "Feature engineering is the process of transforming raw data into meaningful features that improve the performance of machine learning models. It involves creating new features, modifying existing ones, or selecting the most relevant features for the model.\n",
    "\n",
    "Importance:\n",
    "\n",
    "1.Improves Model Performance: Well-engineered features can significantly enhance model accuracy.\n",
    "2.Reduces Complexity: Relevant features reduce the need for complex models.\n",
    "3.Makes the Model Interpretable: It can help make the model more understandable and actionable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "402e076c-b27a-4d91-803f-681c5b627707",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ques 75"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc81487a-3a36-4ad5-b991-b7ec1875ceda",
   "metadata": {},
   "source": [
    "1.Understanding the Data: Analyze the dataset to identify patterns, trends, and relationships.\n",
    "2.Cleaning the Data: Handle missing values, outliers, and incorrect data.\n",
    "3.Creating New Features:\n",
    "Transformation: Apply mathematical operations (e.g., log, square root) to scale or normalize features.\n",
    "Aggregation: Combine features to summarize information (e.g., averages, sums).\n",
    "Decomposition: Break features into meaningful components (e.g., splitting date into year, month, day).\n",
    "4.Encoding Categorical Data: Use methods like one-hot encoding or label encoding for categorical variables.\n",
    "5.Feature Scaling: Normalize or standardize features to ensure they are on a similar scale.\n",
    "6.Feature Selection: Choose the most relevant features using techniques like correlation analysis, model-based methods, or dimensionality reduction.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "145412bb-dcc3-447f-bdfa-660dea60b485",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ques 76"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7e3cd5c-6617-431b-976c-e1d4a1778753",
   "metadata": {},
   "source": [
    "1.Log Transformation: Apply log transformations to skewed numerical features to make them more normally distributed.\n",
    "2.Binning: Convert continuous variables into categorical bins (e.g., age groups).\n",
    "3.Time Series Decomposition: Extract time-based features like day of the week, month, or season.\n",
    "4.Polynomial Features: Create new features by combining existing features (e.g., feature interaction terms).\n",
    "5.Text Vectorization: Convert text data into numerical form using techniques like TF-IDF or Word2Vec.\n",
    "6.Domain-Specific Features: Create features based on domain knowledge, like customer churn predictions using tenure, usage patterns, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb71047d-823c-4975-afcd-bb9cbddb2881",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ques 77"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "738d2ac6-86d9-471d-8632-1275e877d790",
   "metadata": {},
   "source": [
    "Feature Engineering involves creating or transforming new features from raw data, often improving the information content or making it more suitable for machine learning models.\n",
    "Feature Selection is the process of choosing the most relevant features from the engineered features, often using statistical methods or algorithms to eliminate irrelevant or redundant features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0601a28d-2adb-45b7-b02b-df2072d00d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ques 78"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae481858-2293-425f-b916-58e4a01eba3c",
   "metadata": {},
   "source": [
    "Feature selection is crucial in machine learning pipelines for the following reasons:\n",
    "\n",
    "Improves Model Accuracy: By removing irrelevant or redundant features, models focus on the most informative features, which can improve predictive power.\n",
    "Reduces Overfitting: Removing unnecessary features helps in reducing the complexity of the model, leading to better generalization.\n",
    "Decreases Training Time: A smaller feature set speeds up model training and testing.\n",
    "Improves Interpretability: Fewer features make the model easier to understand and interpret."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2344fe4-050f-494e-b88b-dd85367670d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ques 79"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "574d8d6b-8f9d-4e72-85cc-e2dcd73d3114",
   "metadata": {},
   "source": [
    "Positive impact:\n",
    "1.Helps in building simpler, faster models.\n",
    "2.Reduces the risk of overfitting by eliminating noisy or redundant features.\n",
    "3.Improves model generalization, especially with limited data.\n",
    "\n",
    "Negative impact:\n",
    "1.Removing too many features can lead to underfitting, where the model cannot capture the underlying patterns of the data.\n",
    "2.Incorrect feature selection might discard features that contribute significantly to the model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93550361-1956-4a2a-a95e-6db63f2d81e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ques 80"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc104b67-8753-4b94-aae0-bcd9e0799428",
   "metadata": {},
   "source": [
    "Correlation Analysis: Remove features that are highly correlated (e.g., Pearson correlation > 0.9).\n",
    "Statistical Tests: Use tests like Chi-square for categorical variables or ANOVA for continuous variables to assess feature importance.\n",
    "Model-Based Methods: Use tree-based algorithms (e.g., Random Forest) or feature importance methods to identify the most relevant features.\n",
    "Domain Knowledge: Include features that make sense based on your understanding of the problem and dataset.\n",
    "Stepwise Methods: Use techniques like Forward Selection, Backward Elimination, or Recursive Feature Elimination (RFE) to iteratively select features based on their contribution to model performance.\n",
    "Cross-Validation: Use cross-validation to test how different sets of features affect model performance and generalization."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
